# -*- coding: utf-8 -*-
"""Paperspace_work.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aIJd53NJPzd0f9vo9DaiGEnyEX2ac72B
"""

# import os

# os.system('ls')

# os.getcwd()

# # # download COCO, the VQA questions and answers (annotations)
# import os
# from os import path as osp
# dir_raw = './testing/'
# dir_zip = osp.join(dir_raw, 'zip')
# os.system('mkdir -p '+dir_zip)
# dir_ann = osp.join(dir_raw, 'Research')
# os.system('mkdir -p '+dir_ann)

# os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip -P '+dir_zip)
# os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip -P '+dir_zip)
# os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip -P '+dir_zip)
# os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip -P '+dir_zip)
# os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip -P '+dir_zip)
# os.system('unzip '+osp.join(dir_zip, 'v2_Questions_Train_mscoco.zip')+' -d '+dir_ann)
# os.system('unzip '+osp.join(dir_zip, 'v2_Questions_Val_mscoco.zip')+' -d '+dir_ann)
# os.system('unzip '+osp.join(dir_zip, 'v2_Questions_Test_mscoco.zip')+' -d '+dir_ann)
# os.system('unzip '+osp.join(dir_zip, 'v2_Annotations_Train_mscoco.zip')+' -d '+dir_ann)
# os.system('unzip '+osp.join(dir_zip, 'v2_Annotations_Val_mscoco.zip')+' -d '+dir_ann)

# os.system('mv '+osp.join(dir_ann, 'v2_mscoco_train2014_annotations.json')+' '
#                 +osp.join(dir_ann, 'mscoco_train2014_annotations.json'))
# os.system('mv '+osp.join(dir_ann, 'v2_mscoco_val2014_annotations.json')+' '
#                 +osp.join(dir_ann, 'mscoco_val2014_annotations.json'))
# os.system('mv '+osp.join(dir_ann, 'v2_OpenEnded_mscoco_train2014_questions.json')+' '
#                 +osp.join(dir_ann, 'OpenEnded_mscoco_train2014_questions.json'))
# os.system('mv '+osp.join(dir_ann, 'v2_OpenEnded_mscoco_val2014_questions.json')+' '
#                 +osp.join(dir_ann, 'OpenEnded_mscoco_val2014_questions.json'))
# os.system('mv '+osp.join(dir_ann, 'v2_OpenEnded_mscoco_test2015_questions.json')+' '
#                 +osp.join(dir_ann, 'OpenEnded_mscoco_test2015_questions.json'))
# os.system('mv '+osp.join(dir_ann, 'v2_OpenEnded_mscoco_test-dev2015_questions.json')+' '
#                 +osp.join(dir_ann, 'OpenEnded_mscoco_test-dev2015_questions.json'))

# dir_images = osp.join(dir_raw, 'images')
# train = osp.join(dir_images, 'train')
# val = osp.join(dir_images, 'val')
# os.system('mkdir -p '+dir_images)

# os.system('wget http://images.cocodataset.org/zips/train2014.zip -P '+dir_zip)
# os.system('wget http://images.cocodataset.org/zips/val2014.zip -P '+dir_zip)

# copy and unarchive
import os
zip_loc = "/content/gdrive/My Drive/zip/train2014.zip"
os.system('mkdir /content/testData')
os.system('cp '+zip_loc + '/content/testData/train2014.zip')
os.system('unzip /content/testData/train2014.zip')

# os.system('unzip '+osp.join(dir_zip, 'train2014.zip.1')+' -d ' + train)

# !du images

# os.system('unzip '+osp.join(dir_zip, 'val2014.zip.2')+' -d ' + val)

# from pathlib import Path
# path = Path("/content/gdrive/My Drive/Research")
# folder = "images1"
# dest = path/folder
# dest.mkdir(parents=True, exist_ok=True)

# import shutil

# my_file = Path('./images')
# to_file = dest/folder
# shutil.copytree(my_file, to_file)

from google.colab import drive 
drive.mount('/content/gdrive')

!pip install pytorch-lightning-bolts
!pip install pytorch
!pip install pytorch-lightning

from torchvision.models import resnet18
from torchvision import transforms
from torch.utils import data
import torchvision
from torch.utils.data import Dataset, DataLoader

import os
import json
from PIL import Image
from torch.utils.data import Dataset
import string
from tqdm import tqdm
import torch

class JeopardyDataset(Dataset):
    """Face Landmarks dataset."""

    def __init__(self, questions_file, answers_file, images_dir, transform):
        """
        Args:
            questions_file (string): Path to the json file with questions.
            answers_file (string): Path to the json file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        q_f = open(questions_file, 'r')
        a_f = open(answers_file, 'r')
        
        self.questions = json.load(q_f)["questions"]
        self.answers = json.load(a_f)["annotations"]
        self.images_dir = images_dir
        self.image_id_to_filename = self._find_images()
        self.transform = transform
        self.question_to_answer = self._find_answers()
        self.mega_dict = {} # this will store our indices
        self.numerical = {}

        answer_text = ' . '.join(list(self.question_to_answer.values()))
        question_text = ' . '.join([q['question'] for q in self.questions])
        question_text = question_text.replace("?", " END")
        # strip all punctuation
        question_text = question_text.translate(str.maketrans('', '', string.punctuation))
        tokens = question_text.split(' ')
        tokens.extend(answer_text.split(' '))
        vocab = set(tokens)
        vocab_length = len(vocab)
        print(vocab_length)
        print("vocab length")
        vocab = list(vocab)
        word2idx = {w:i for i,w in enumerate(vocab)}
        word2idx["DUMMY"] = len(vocab)
        
        index = 0
        for question in tqdm(self.questions):
          arr = question['question']
          arr = arr.replace("?", " END").translate(str.maketrans('', '', string.punctuation)).split()
          # dataset reduced by only 50%, and RNN is easier, don't have to worry about padding now
          if len(arr) < 6 or len(arr) > 7:
            continue
          # add padding to arr, and separate the question mark
          if len(arr) == 6:
            arr.append("DUMMY")
          # len is always 7
          if question["image_id"] in self.image_id_to_filename and question['question_id'] in self.question_to_answer:
              self.numerical[question["question_id"]] = [word2idx[word] for word in arr]
              self.mega_dict[index] = question, self.image_id_to_filename[question["image_id"]], word2idx[self.question_to_answer[question["question_id"]]]
              if index < 10:
                  print("index is " + str(index))
                  print(self.mega_dict[index])
              index += 1

        self.index = index
        print("length is " + str(index))

    def _find_answers(self):
        # TODO: deal with the complications - there several answers for each question, 
        # here's entry 0: {"answer": "net", "answer_confidence": "maybe", "answer_id": 1}
        question_to_answer = {}
        for ann in self.answers:
            for answer in ann['answers']:
              actual_ans = answer['answer']
              if answer['answer_confidence'] == 'yes' and len(actual_ans.split()) == 1:
                # whatever the first yes is, scrappy model; assuming every field has at least one yes answer
                question_to_answer[ann["question_id"]] = actual_ans
        return question_to_answer

    def _find_images(self):
        id_to_filename = {}
        
        for filename in os.listdir(self.images_dir):
            if not filename.endswith('.jpg'):
                continue
            id_and_extension = filename.split('_')[-1]
            image_id = int(id_and_extension.split('.')[0])
            id_to_filename[image_id] = filename
            
        return id_to_filename


    def __len__(self):
        return self.index

    def __getitem__(self, idx):
        question, image, answer = self.mega_dict[idx]
        path = os.path.join(self.images_dir, self.image_id_to_filename[question["image_id"]])
        img = self.transform(Image.open(path).convert('RGB'))
        if len(self.numerical[question['question_id']]) != 7:
          print("something wrong")
        # convert question to numerical representation here?
        return self.numerical[question['question_id']], img, answer

# example call
# test_ds = JeopardyDataset("v2_OpenEnded_mscoco_train2014_questions.json", "v2_mscoco_train2014_annotations.json", "images")

import torch.nn.functional as F

def l2_normalize(x, dim=1):
    return x / torch.sqrt(torch.sum(x**2, dim=dim).unsqueeze(dim))

class SimCLR(object):

    def __init__(self, outputs1, outputs2, t=0.07, **kwargs):
        super().__init__()
        self.outputs1 = l2_normalize(outputs1, dim=1)
        self.outputs2 = l2_normalize(outputs2, dim=1)
        self.t = t

    def get_loss(self):
        witness_pos = torch.sum(self.outputs1 * self.outputs2, dim=1)
        outputs12 = torch.cat([self.outputs1, self.outputs2], dim=0)
        witness_partition = self.outputs1 @ outputs12.T
        witness_partition = torch.logsumexp(witness_partition / self.t, dim=1)
        loss = -torch.mean(witness_pos / self.t - witness_partition)
        return loss

from pl_bolts.models.self_supervised.evaluator import Flatten
import torch.nn.functional as F
from torch.optim import SGD, Adam
from pl_bolts.models.regression import LogisticRegression
import pytorch_lightning as pl
from torch.nn import Linear, Tanh, LSTM, Embedding

class JeopardyModel(pl.LightningModule):
    def __init__(self, vocab_sz, bs):
      super().__init__()
      im_vec_dim = 128
      ans_dim = 128
      question_dim = 256
      n_hidden = 100
      n_layers = 1
      
      self.i_h = Embedding(vocab_sz, n_hidden)
      self.rnn = LSTM(n_hidden, n_hidden)
      self.h_o = Linear(n_hidden, question_dim)
      # make it None
      self.h = None
      self.ans_final = Linear(n_hidden, ans_dim)

      # for images, we use resnet18 pretrained
      self.image_feature_extractor = resnet18(pretrained=True, progress=True)
      for p in self.image_feature_extractor.parameters():
        p.requires_grad = False

      self.image_feature_extractor.fc = Linear(512, im_vec_dim)
      
    def forward(self, x):
      # check which layers are getting frozen and which are not
      return self.image_feature_extractor(x)

    def forward_question(self, x):
      # we have a question as input
      if not self.h:
        self.h = torch.zeros(1, 16, 100, device=self.device), torch.zeros(1, 16, 100, device=self.device) # h0, c0
      res, h = self.rnn(self.i_h(x), self.h)
      self.h = h[0], h[1] # both h_n and c_n are getting detached?
      return self.h_o(res)

    def forward_answer(self, x):
      x = self.i_h(x)
      return self.ans_final(x)
        

    def training_step(self, batch, batch_idx):
      loss = self.shared_step(batch)
      result = pl.TrainResult(loss)
      result.log_dict({'train_loss': loss}, prog_bar=True)
      return result

    def validation_step(self, batch, batch_idx):
      loss = self.shared_step(batch)
      result = pl.EvalResult(checkpoint_on=loss, early_stop_on=loss)
      result.log_dict({'val_loss': loss}, prog_bar=True)
      return result

    def test_step(self, batch, batch_idx):
      loss = self.shared_step(batch)
      result = pl.EvalResult()
      result.log_dict({'test_loss': loss}, prog_bar=True)
      return result

    def shared_step(self, batch):
      question, image, answer = batch
      question = torch.stack(question)
      f_q = self.forward_question(question)
      f_q = f_q.squeeze()[-1, :]
      f_a = self.forward_answer(answer)
      im_vector = self(image)
      answer_image_vector = torch.cat((f_a, im_vector), 1)
      loss = SimCLR(answer_image_vector, f_q).get_loss()
      return loss


    def configure_optimizers(self):
      # will be taking outputs1, outputs2 as params where output 1 is the concat
      return SGD(self.image_feature_extractor.fc.parameters(), lr=2e-2) # find out how to make this so that every thing is optimizable??

questions_file = "/content/gdrive/My Drive/zip/questions_train/v2_OpenEnded_mscoco_train2014_questions.json"
answers_file = "/content/gdrive/My Drive/zip/annotations_train/v2_mscoco_train2014_annotations.json"

# copy and then unarchive instead?
coco_loc = "/content/gdrive/My Drive/Research/images1/images1/train/train2014"
img_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
dataset = JeopardyDataset(questions_file, answers_file, coco_loc, img_transform)

dataset[3254]
# import string
# questions = open(questions_file, 'r')
# questions = json.load(questions)["questions"]
# question_text = ' . '.join([q['question'] for q in questions])
# question_text = question_text.replace("?", " ?")
# # question_text[:100]
# tokens = question_text.split(' ')
# vocab = set(tokens)
# vocab = list(vocab)
# word2idx = {w:i for i,w in enumerate(vocab)}
# # nums is going to be defined in every question

# # so in the dataset __getitem__ function, I'd return nums[independent, dependent]
# # these must always be the same size if I'm using an LSTM

from torch.utils.data.sampler import SubsetRandomSampler

trainer = pl.Trainer(max_epochs=1, progress_bar_refresh_rate=50, gpus=1)
batch_size = 16
validation_split = .2
shuffle_dataset = True
random_seed= 42

# Creating data indices for training and validation splits:
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(validation_split * dataset_size)

train_indices, val_indices = indices[split:], indices[:split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)

train_loader = DataLoader(dataset, batch_size=batch_size, 
                                           sampler=train_sampler, pin_memory=True)
val_loader = DataLoader(dataset, batch_size=batch_size,
                                                sampler=valid_sampler, pin_memory=True)

from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
model = JeopardyModel(22805, batch_size)
trainer.fit(model, train_loader, val_loader)

# from pl_bolts.models.self_supervised.evaluator import Flatten
# import torch.nn.functional as F
# from torch.optim import SGD, Adam
# from pl_bolts.models.regression import LogisticRegression
# import pytorch_lightning as pl
# from torch.nn import Linear, Tanh, LSTM, Embedding
# from sentence_transformers import SentenceTransformer, models

# class JeopardyModel(pl.LightningModule):
#     def __init__(self, vocab_sz, sentence_embed=False):
#       super().__init__()
#       im_vec_dim = 128
#       ans_dim = 128
#       question_dim = 256
#       n_hidden = 100
#       n_layers = 2
#       # self.sentence_embed = sentence_embed
#       # if self.sentence_embed:
#       #   # need to modify the dataset as well for this to work, takes in string input
#       #   word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=10)
#       #   pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
#       #   dense_model_a = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=ans_dim, activation_function=Tanh())
#       #   dense_model_q = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=question_dim, activation_function=Tanh())

#       #   self.q_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model_q])
#       #   self.a_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model_a])

#       #   # TODO: do this the lightning way, it's cleaner
#       #   for p in self.q_model.parameters():
#       #     p.requires_grad = False
#       #   for p in self.a_model.parameters():
#       #     p.requires_grad = False
#       else:
#         self.i_h = Embedding(vocab_sz, n_hidden)
#         self.rnn = LSTM(n_hidden, n_hidden)
#         self.h_o = Linear(n_hidden, question_dim)
#         self.h = [torch.zeros(n_layers, n_hidden) for _ in range(2)]
#         self.ans_emb_layer = Embedding(ans_vocab_sz, n_hidden)
#         self.ans_final = Linear(ans_vocab_sz, ans_dim)

#       # for images, we use resnet18 pretrained
#       self.image_feature_extractor = resnet18(pretrained=True, progress=True)
#       for p in self.image_feature_extractor.parameters():
#         p.requires_grad = False

#       self.image_feature_extractor.fc = Linear(512, im_vec_dim)
      
#     def forward(self, x):
#       # check which layers are getting frozen and which are not
#       return self.image_feature_extractor(x)

#     def forward_question(self, x):
#       # we have a question as input
#       if self.sentence_embed:
#         return self.q_model.encode(x)
#       else:
#         res, h = self.rnn(self.i_h(x), self.h)
#         self.h = [h_.detach() for h_ in h]
#         return self.h_o(res)

#     def forward_answer(self, x):
#       if self.sentence_embed:      
#         return self.a_model.encode(x)
#       else:
        

#     def training_step(self, batch, batch_idx):
#       loss = self.shared_step(batch)
#       result = pl.TrainResult(loss)
#       result.log_dict({'train_loss': loss}, prog_bar=True)
#       return result

#     def validation_step(self, batch, batch_idx):
#       loss = self.shared_step(batch)
#       result = pl.EvalResult(checkpoint_on=loss, early_stop_on=loss)
#       result.log_dict({'val_loss': loss}, prog_bar=True)
#       return result

#     def test_step(self, batch, batch_idx):
#       loss = self.shared_step(batch)
#       result = pl.EvalResult()
#       result.log_dict({'test_loss': loss}, prog_bar=True)
#       return result

#     def shared_step(self, batch):
#       question, image, answer = batch
#       f_q = torch.from_numpy(self.forward_question(question)).cuda()
#       f_a = torch.from_numpy(self.forward_answer(answer)).cuda()
#       im_vector = self(image).cuda()
#       answer_image_vector = torch.cat((f_a, im_vector), 1)
#       loss = SimCLR(answer_image_vector, f_q).get_loss()
#       return loss

#     def configure_optimizers(self):
#       # will be taking outputs1, outputs2 as params where output 1 is the concat
#       return SGD(self.image_feature_extractor.fc.parameters(), lr=2e-2) # find out how to make this so that every thing is optimizable??



# arr = [q['question'] for q in questions]
# longest = 0
# avg = 0
# question_lengths = [0 for _ in range(23)]
# for q in arr:
#   a = len(q.split())
#   avg += a
#   question_lengths[a] += 1
#   if a > longest:
#     longest = a
#     print(q)
# print(avg/len(arr))
# print(question_lengths)
# # strip all punctuation except question mark?
# import matplotlib.pyplot as plt
# fig = plt.figure()
# ax = fig.add_axes([0,0,1,1])
# langs = [_ for _ in range(23)]
# ax.bar(langs, question_lengths)
# ax.pie(question_lengths,labels=langs)
# plt.show()
# # 5 + 6 are together about 50% of the dataset

